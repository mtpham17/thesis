---
title: "simulation_scratchwork"
author: "Thu Pham"
date: "2022-11-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/Users/thupham/Desktop/senior-yr/thesis/elsah-coptop')
library(readstata13)
library(tidyverse)
library(glmnet)
library(caret)
library(fastDummies)
library(mvtnorm)
library(dplyr)
library(glmnet)
library(glmmLasso)
```


```{r}
# helper functions


# need to find a minimum lambda value that we can start from, since too small of a lambda value returns a message about the matrix not being invertible
# min_lambda <- function(data, formula, rand=list(j=~1)) {
#   failed <- TRUE
#   lambda_min <- 0.01
#   repeat {
# 
#   failed <- tryCatch(
#       {
#         glmmLasso(fix = formula, rnd = rand, data = data, 
#             lambda = lambda_min)
#       },
#       error = function(e){
#           TRUE
#       }
#    )
#     if (is.logical(failed)) {
#       lambda_min <- lambda_min + 0.01
#     }
#     
#     else {
#       break
#     }
#   }
#   return(lambda_min)
# }
```


```{r}
# cross validation helper function for the glmmLasso
# need to eventually do this with k-folds, or get as close to the 
# cross validation that is happening in the regular lasso

library(MASS)
library(nlme)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# https://rdrr.io/cran/glmmLasso/src/demo/glmmLasso-soccer.r
cv_glmmLasso <- function(data, formula, rand=list(j=~1),
                         lambda_step=100) {
  # fixing the min lambda for now -- let's see if that solve our problem
  # min_lambda <- 0.001 
  
  # lambdas <- seq(from=min_lambda, to=10, 
                 # by=(10 - min_lambda) / lambda_step)
  lambdas <- 10 ^ seq(10,-2,length = 100)
  # print(lambdas)
  
  N <- dim(data)[1]
  ind <- sample(N, N)
  
  # number of folds
  
  kk <- 5
  nk <- floor(N/kk)
  
  pred_error <- matrix(NA, ncol=kk, nrow = lambda_step)

  
  for (j in 1:lambda_step) {
    # print(paste("Iteration", j))
    
    for (i in 1:kk)
    {
      if (i < kk) {
      indi <- ind[(i-1)*nk+(1:nk)]
      }
      else {
      indi <- ind[((i-1)*nk+1):N]
      }
  
    data_train <- data[-indi,]
    data_test <-data[indi,]
  
    glm <- try(glmmLasso(fix = formula, rnd = rand, data = data_train, 
              lambda = lambdas[j]), silent = TRUE) 
            
        if(!inherits(glm, "try-error"))
        {  
          y_hat<-predict(glm, data_test)  
          pred_error[j,i]<- sqrt(sum((data_test$Y - y_hat)^2) / nrow(data_test))
        }
    }
  }
  
  # find lambda which gives lowest prediction error
  pred_error_vec <- apply(pred_error, 1, sum)
  # print(pred_error_vec)
  # print(pred_error_vec)
  # print(paste("min lambda:", lambdas[which.min(pred_error_vec)]))
  return(lambdas[which.min(pred_error_vec)])  
}
```


```{r}
# beta generator function, following Trevor Hastie, Robert Tibshirani, Ryan Tibshirani

# we are excluding the intercept from this definition of beta


# type = one of c(1, 2, 3, 5) -- to specify the type of beta vector we will have.
# p = number of predictors (also the number of betas)
# s = number of beta's that follow the setup given in the paper
# default = value that the s components take on

# beta-type 1: \beta's have s components equal to (default), occurring at (roughly) equal spaced indices between 1 and p, and the rest equal to 0
# beta-type 2: \beta has its first s components equal to 1, and the rest equal to 0
# beta-type 3: \beta has its first s components, taking nonzero values equally spaced between 10 and 0.5, and the rest equal to 0
# added back in beta-type 4 (the Ryan paper excluded it because they got similar results, but I'm running a slightly different simulation): \beta has its first s components, taking nonzero values equally spaced between -10 and 10, and the rest equal to 0 
# beta-type 5: \beta has its first s components equal to 1, and the rest decaying exponentially to 0, specifically \beta_i = 0.5^{i - s}, for i = s + 1, ..., p

generate_beta <- function(type, p, s, default = 1) {
  if (type == 1) {
    # create roughly spaced indices, we want s betas to have a value of 1
    indices <- seq(from = 1, to = p, 
                   by = ceiling(p / s))
    betas <- rep(NA, p)
    betas[indices] <- 1
    betas[-indices] <- 0
    return(betas)
  }
  
  if (type == 2) {
    return(c(rep(default, s), rep(0, p - s)))
  }
  
  if (type == 3) {
    return(c(seq(from = -10, to = 0.5, by = 20 / s), 
             rep(0, p - s)))
  }
  
  if (type == 4) {
    return(c(seq(from = -10, to = 10, by = 20 / s), 
             rep(0, p - s)))

  }
  
  return(c(rep(default, s), 0.5^seq(s + 1, p)))
  
}
```

```{r rmse}
rmse <- function(predicted, true, n) {
  return(sqrt(sum((predicted - true)^2) / n))
}
```

```{r generate_data, warnings=FALSE}

## DESCRIPTION OF INPUTS
# type, p, s, default are all the arguments that are taken in by generate_beta
# cov_x = covariance between covariates
# mean_x = mean of covariates
# sigma_x = standard deviation of covariate distribution
# cov_rand_fixed = covariance between clusters and fixed beta coefficients (will probably have to modify the generate_beta function? may be a question for Luke)
# mean_r = mean of random intercepts
# sigma_r = standard deviation of random intercepts

## TODO: NEED TO INTRODUCE SOME WAY TO CORRELATE THE CLUSTERS WITH THE BETA COEFFICIENTS

## TODO: NEED TO FIND SOME WAY TO MAKE UNEVENLY SIZED CLUSTERS

# should the noise due to measurement have correlations between them? probably not right?

generate_data <- function(j, n = 100, 
                          type, p, s, default,
                          cov_x, mean_x = 0, sigma_x = 1, 
                          cov_rand_fixed,
                          mean_r = 0, sigma_r, 
                          mean_noise = 0, sigma_noise = 1) {
  colnames_df <- c(paste("X", 1:p, sep = ""), "random_beta") 
  random_beta <- rnorm(n=j, mean=0, sigma_r)
  
  # make some of these zero
  fixed_beta <- generate_beta(type, p, s, default)
  
  sigma <- diag(sigma_x^2, p, p)
  sigma[outer(1:p, 1:p, function(i,j) i!=j)] <- cov_x
  x_all <- rmvnorm(n = n, mean = rep(mean_x, p), sigma)
  
  data <- as.data.frame(cbind(x_all, rep(random_beta, each = n/j)))
  colnames(data) <- colnames_df
  
  # error term 
  epsilon <- rnorm(n)
  
  # now, relaxing the assumption of how strong our measures are. per Luke's suggestion,
  # add the noise into the covariates so that we preserve the structure from earlier.
  
  ## TODO: somehow need to incorporate the \mu_j into the \mu_{ij} (per Luke's suggestion)
  
  measurement_noise <- rmvnorm(n = n, mean = rep(0, p), diag(sigma_noise^2, p, p))
  measured_x <- x_all + measurement_noise
  
  data$Y <- rowSums(sweep(measured_x, 2, fixed_beta, "*")) + random_beta + epsilon
  data$j <- as.factor(rep(1:j, each=n/j))
  # print("generated data")
  return(data)
}

test_df <- generate_data(j = 2, n = 100, 
              type = 1, p = 5, s = 3, default = 1, 
              cov_x = 0.5, mean_x = 0, sigma_x = 1, 
              cov_rand_fixed = 0, mean_r = 0, sigma_r = 2)

```


```{r}
# Analyze
analyze <- function(data, p, fixed_beta) {
  # need to do cross validation for LASSO
  var.names <- paste("X", 1:p, sep = "")
  com.names <- lapply(seq_along(var.names),
                    function(i) combn(var.names, i, FUN = paste, collapse = " + "))
  covariates <- com.names[[p]]
  X_fixed <- model.matrix(as.formula(paste("Y ~", covariates)),
                          data = data)
  X_fixed <- X_fixed[, -1]
  Y <- data$Y

  # call cv.glmnet()
  model_lasso_fixed <- cv.glmnet(x = X_fixed, y = Y, nfolds = 5, alpha = 1)
  cc <- coef(model_lasso_fixed, s = model_lasso_fixed$lambda.min)
  #placeholder lambda value
  coefficients_fixed <- coef(model_lasso_fixed)
  
  
  lambda_min <- cv_glmmLasso(data = data,
                             formula = as.formula(paste("Y ~", covariates)))
  model_lasso_mixed <- glmmLasso(fix = as.formula(paste("Y ~", covariates)),
                                                     rnd=list(j=~1),
                                                    data = data,
                                                     lambda=lambda_min)

  coefficients_mixed <- model_lasso_mixed$coefficients
  coefficients <- cbind(as.matrix(coefficients_fixed), as.matrix(coefficients_mixed))
  colnames(coefficients) <- c("fixed", "mixed")
  coefs_df <- as.data.frame(coefficients[2:nrow(coefficients), ])
  
  # report R^2 as a measure of signal to noise ratio
  
  basic_linear <- lm(as.formula(paste("Y ~", covariates)), data = data)
  
  # "the two models agreed on ___ percent of the covariates" -- agreement defined as they both 
  # deemed a beta as non-zero out of the total significant betas. not really sure if this
  # is legitimate though
  
  agreement <- sum((coefs_df$fixed != 0 & coefs_df$mixed != 0) |
                  (coefs_df$fixed == 0 & coefs_df$mixed == 0)) / p
  
  # also want to measure the accuracy of both -- root mean squared error of 
  # beta hat and beta_true
  
  fixed_beta_rmse <- rmse(coefs_df$fixed, fixed_beta, nrow(coefs_df))
  mixed_beta_rmse <- rmse(coefs_df$mixed, fixed_beta, nrow(coefs_df))
  
  # measure whether the two reported non-zero coefficients when they were supposed to
  # percentage of total non-zero betas that were correctly deemed as significant
  
  fixed_accuracy <- sum(coefs_df$fixed != 0 & fixed_beta != 0) / sum(fixed_beta != 0)
  mixed_accuracy <- sum(coefs_df$mixed != 0 & fixed_beta != 0) / sum(fixed_beta != 0)
  
  # mean squared error of both models
  y_hat_fixed <- predict(model_lasso_fixed, newx = X_fixed, s = model_lasso_fixed$lambda.min)
  y_hat_mixed <- predict(model_lasso_mixed, newx = X_fixed, s = lambda_min)
  
  fixed_rmse <- rmse(y_hat_fixed, data$Y, nrow(data))
  mixed_rmse <- rmse(y_hat_mixed, data$Y, nrow(data))
  
  
  return(c(summary(basic_linear)$adj.r.squared, 
           agreement, fixed_beta_rmse, mixed_beta_rmse, fixed_accuracy, mixed_accuracy,
           fixed_rmse, mixed_rmse))
}

test_beta <- generate_beta(1, 5, 3)
analyze(test_df, 5, test_beta)

```


```{r}
# Repeat

one_run <- function(j, n = 100, 
                    type, p, s, default,
                    cov_x, mean_x = 0, sigma_x = 1,
                    cov_rand_fixed = 0,
                    mean_r = 0, sigma_r,
                    mean_noise = 0, sigma_noise = 1) {
  
  dat <- generate_data(j, n, type, p, s, default,cov_x, mean_x, sigma_x,
                       cov_rand_fixed, mean_r, sigma_r, mean_noise, sigma_noise)
  fixed_beta <- generate_beta(type, p, s, default)
  # may need to fix this
  result <- analyze(dat, p, fixed_beta)
  # feed back number of predictors, number of clusters, type of fixed beta coefficients (from paper)
  # the default value of beta, sigma of the random intercepts, number of 
  return(c(j, n, type, p, s, default, cov_x, mean_x, sigma_x,
           cov_rand_fixed, mean_r, sigma_r, mean_noise, 
           sigma_noise, result))
}

# sigma_r -- clusters are further and further apart
# first step -- "the clusters are just adding noise" -- have weak beta (slopes) and alter sigma_r
# eventually work up to using covariates directly from data
# next step -- correlate the intercepts with the covariates -- do this after you get the whole simulation going

# if you keep getting weird results with the simulation, email dataset and short script to groll@statistik.tu-dortmund.de

rerun_single_params <- function(df) {
  # print("starting an iteration")
  # print(df[7])
  
  results <- rerun(unname(df[1]), one_run(j = unname(df[2]), n = unname(df[3]), 
                                        type = unname(df[4]), p = unname(df[5]), 
                                        s = unname(df[6]), default = unname(df[7]),
                                        cov_x = unname(df[8]), mean_x = unname(df[9]), 
                                        sigma_x = unname(df[10]), cov_rand_fixed = unname(df[11]),
                                        mean_r = unname(df[12]), sigma_r = unname(df[13]),
                                        mean_noise = unname(df[14]), sigma_noise = unname(df[15])))
  
  # return statement of one_run: return(c(j, n, type, p, s, default, cov_x, mean_x, sigma_x,
  #          cov_rand_fixed, mean_r, sigma_r, mean_noise, 
  #          sigma_noise, result))
  
  # return statement of analyze: return(c(agreement, fixed_beta_rmse, mixed_beta_rmse, 
  # fixed_accuracy, mixed_accuracy,
  #        fixed_rmse, mixed_rmse))
  
  # these are subject to change, but just so we're not hard coding anything for now?
  num_metrics <- 7
  num_inputs <- 14
  
  mean_adj_r_squared <- mean(unlist(lapply(results, function (x) x[num_inputs + 1])))
  sd_adj_r_squared <- sd(unlist(lapply(results, function (x) x[num_inputs + 1])))
    
  mean_agreement <- mean(unlist(lapply(results, function (x) x[num_inputs + 2])))
  sd_agreement <- sd(unlist(lapply(results, function (x) x[num_inputs + 2])))
  
  mean_beta_rmse_fixed <- mean(unlist(lapply(results, function (x) x[num_inputs + 3])))
  sd_beta_rmse_fixed <- sd(unlist(lapply(results, function (x) x[num_inputs + 3])))
  
  mean_beta_rmse_mixed <- mean(unlist(lapply(results, function (x) x[num_inputs + 4])))
  sd_beta_rmse_mixed <- sd(unlist(lapply(results, function (x) x[num_inputs + 4])))
  
  mean_accuracy_fixed <- mean(unlist(lapply(results, function (x) x[num_inputs + 5])))
  sd_accuracy_fixed <- sd(unlist(lapply(results, function (x) x[num_inputs + 5])))
  
  mean_accuracy_mixed <- mean(unlist(lapply(results, function (x) x[num_inputs + 6])))
  sd_accuracy_mixed <- sd(unlist(lapply(results, function (x) x[num_inputs + 6])))
  
  mean_rmse_fixed <- mean(unlist(lapply(results, function (x) x[num_inputs + 7])))
  sd_rmse_fixed <- sd(unlist(lapply(results, function (x) x[num_inputs + 7])))
  
  mean_rmse_mixed <- mean(unlist(lapply(results, function (x) x[num_inputs + 8])))
  sd_rmse_mixed <- sd(unlist(lapply(results, function (x) x[num_inputs + 8])))
  
  row <- c(unname(df[1]), unname(df[2]), unname(df[3]), 
          unname(df[4]), unname(df[5]), 
          unname(df[6]), unname(df[7]),
          unname(df[8]), unname(df[9]), 
          unname(df[10]), unname(df[11]),
          unname(df[12]), unname(df[13]),
          unname(df[14]), unname(df[15]), 
          mean_adj_r_squared, sd_adj_r_squared,
          mean_agreement, sd_agreement, 
          mean_beta_rmse_fixed, sd_beta_rmse_fixed,
          mean_beta_rmse_mixed, sd_beta_rmse_mixed,
          mean_accuracy_fixed, sd_accuracy_fixed,
          mean_accuracy_mixed, sd_accuracy_mixed,
          mean_rmse_fixed, sd_rmse_fixed,
          mean_rmse_mixed, sd_rmse_mixed)
  
  names(row) <- c("reps", "j", "n", "type", "p", "s", "default", "cov_x", "mean_x", 
                  "sigma_x", "cov_rand_fixed", "mean_r", "sigma_r", "mean_noise", "sigma_noise",
                  "mean_adj_r2", "sd_adj_r2",
                  "mean_agreement", "sd_agreemeent",
                  "mean_beta_rmse_fixed", "sd_beta_rmse_fixed",
                  "mean_beta_rmse_mixed", "sd_beta_rmse_mixed",
                  "mean_accuracy_fixed", "sd_accuracy_fixed",
                  "mean_accuracy_mixed", "sd_accuracy_mixed",
                  "mean_rmse_fixed", "sd_rmse_fixed",
                  "mean_rmse_mixed", "sd_rmse_mixed")

  return(row)
}

rerun_mult_parameters <- function(runs_vec, j_vec, n_vec, type_vec, p_vec,
                                  s_vec, default_vec, cov_x, mean_x_vec, sigma_x_vec,
                                  cov_rand_fixed_vec,
                                  mean_r_vec, sigma_r_vec,
                                  mean_noise_vec, sigma_noise_vec) {
  
  combos <- expand.grid(runs = runs_vec, j = j_vec, n = n_vec, type = type_vec, p = p_vec,
                        s = s_vec, default_beta = default_vec, cov_x = cov_x, 
                        mean_x = mean_x_vec, sigma_x = sigma_x_vec,
                        cov_rand_fixed = cov_rand_fixed_vec,
                        mean_r = mean_r_vec, sigma_r = sigma_r_vec,
                        mean_noise = mean_noise_vec, sigma_noise = sigma_noise_vec)
  
  results <- apply(combos, 1, rerun_single_params)
  print("finished one set of parameters")
  
  return(results)
  
}

# test_results <- rerun_mult_parameters(runs_vec = 2, m_vec = c(2, 4), j_vec = 4,
#                       beta_strength_vec = 0.5, min_beta_vec = 0.4, 
#                       sigma_r_vec = 0.2, n_vec = 100)



# Summarize
assess_performance <- function(results) {
  # stuff
  return(as.data.frame(t(results)))
}
```

```{r}
options(warn=-1)
set.seed(17)

prelim_results <- rerun_mult_parameters(runs_vec = 5,
                                        j_vec = 2, n_vec = 100,
                                        type_vec = 2,
                                        p_vec = 4, s_vec = 2, default_vec = 1,
                                        cov_x = 0.5, mean_x_vec = 0, sigma_x_vec = 1,
                                        cov_rand_fixed_vec = 0,
                                        mean_r_vec = 0, sigma_r_vec = 1,
                                        mean_noise_vec = 0, sigma_noise_vec = 1)
```


```{r}
# assess_performance(prelim_results)

# prelim_results <- rerun_mult_parameters(runs_vec = 5, 
#                                         j_vec = c(2, 5), n_vec = 100, 
#                                         type_vec = c(1, 2, 3, 4, 5),
#                                         p_vec = c(4, 8), s_vec = 2, default_vec = 1, 
#                                         cov_x = c(0, 0.5), mean_x_vec = 0, sigma_x_vec = 1,
#                                         cov_rand_fixed_vec = 0,
#                                         mean_r_vec = 0, sigma_r_vec = 1,
#                                         mean_noise_vec = 0, sigma_noise_vec = 1)
```
```{r}
df_results <- assess_performance(prelim_results)
df_results$mixed_better <- df_results$mean_acc_fixed > df_results$mean_acc_mixed
df_results[df_results$mean_agreement == min(df_results$mean_agreement), ]
# looking at minimum agreement: larger amounts of clusters, max sigma_r values
# but the mixed model isn't necessarily more accurate, mostly pretty weak coefficients of fixed effects (also makes sense!)
df_results[df_results$mean_agreement == max(df_results$mean_agreement), ]
# description of max
# mix of small and large clusters (mix of different j values), lowest sigma_r values (makes sense!)
# when the two agree the most (100%) agreement, the mixed model is more accurate (lower error) in all cases
```

