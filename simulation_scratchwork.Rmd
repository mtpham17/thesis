---
title: "simulation_scratchwork"
output: pdf_document
date: '2023-01-12'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/Users/thupham/Desktop/senior-yr/thesis/elsah-coptop')
library(readstata13)
library(tidyverse)
library(glmnet)
library(caret)
library(fastDummies)
library(mvtnorm)
library(dplyr)
library(glmnet)
library(glmmLasso)
library(future)
library(furrr)
```

```{r}
# cross validation helper function for the glmmLasso
# need to eventually do this with k-folds, or get as close to the 
# cross validation that is happening in the regular lasso

library(MASS)
library(nlme)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# https://rdrr.io/cran/glmmLasso/src/demo/glmmLasso-soccer.r
cv_glmmLasso <- function(data, formula, rand=list(j=~1),
                      lambda_step=100) {
  # fixing the min lambda for now -- let's see if that solve our problem
  # min_lambda <- 0.001 
  
  # lambdas <- seq(from=min_lambda, to=10, 
                 # by=(10 - min_lambda) / lambda_step)
  lambdas <- 10 ^ seq(5,-2,length = 20)
  # print(lambdas)
  
  N <- dim(data)[1]
  ind <- sample(N, N)
  
  # number of folds
  
  kk <- 5
  nk <- floor(N/kk)
  
  pred_error <- matrix(NA, ncol=kk, nrow = lambda_step)

  
  for (j in 1:lambda_step) {
    # print(paste("Iteration", j))
    
    for (i in 1:kk)
    {
      if (i < kk) {
      indi <- ind[(i-1)*nk+(1:nk)]
      }
      else {
      indi <- ind[((i-1)*nk+1):N]
      }
  
    data_train <- data[-indi,]
    data_test <-data[indi,]
  
    glm <- try(glmmLasso(fix = formula, rnd = rand, data = data_train, 
              lambda = lambdas[j]), silent = TRUE) 
            
        if(!inherits(glm, "try-error"))
        {  
          y_hat<-predict(glm, data_test)  
          pred_error[j,i]<- sqrt(sum((data_test$Y - y_hat)^2) / nrow(data_test))
        }
    }
  }
  
  # find lambda which gives lowest prediction error
  pred_error_vec <- apply(pred_error, 1, sum)
  # print(pred_error_vec)
  # print(pred_error_vec)
  # print(paste("min lambda:", lambdas[which.min(pred_error_vec)]))
  return(lambdas[which.min(pred_error_vec)])  
}

## run a simulation with a lot of lambdas, get a range of lambdas that we consistently see across these simulations. restrict the range based on those results
```

```{r}
gcd <- function(x,y) {
  r <- x %% y;
  return(ifelse(r, gcd(y, r), y))
}

```

```{r}
# beta generator function, following Trevor Hastie, Robert Tibshirani, Ryan Tibshirani

# we are excluding the intercept from this definition of beta


# type = one of c(1, 2, 3, 5) -- to specify the type of beta vector we will have.
# p = number of predictors (also the number of betas)
# s = number of beta's that follow the setup given in the paper
# default = value that the s components take on

# beta-type 1: \beta's have s components equal to (default), occurring at (roughly) equal spaced indices between 1 and p, and the rest equal to 0
# beta-type 2: \beta has its first s components equal to 1, and the rest equal to 0
# beta-type 3: \beta has its first s components, taking nonzero values equally spaced between 10 and 0.5, and the rest equal to 0
# added back in beta-type 4 (the Ryan paper excluded it because they got similar results, but I'm running a slightly different simulation): \beta has its first s components, taking nonzero values equally spaced between -10 and 10, and the rest equal to 0 
# beta-type 5: \beta has its first s components equal to 1, and the rest decaying exponentially to 0, specifically \beta_i = 0.5^{i - s}, for i = s + 1, ..., p

generate_beta <- function(type, p, s, default = 1) {
  if (s > p) {
    s <- p
  }
  if (type == 1) {
    # create roughly spaced indices, we want s betas to have a value of 1
    
    indices <- seq(1, p, length.out = s)
    betas <- rep(0, p)
    betas[indices] <- 1
    # if (sum(betas) != s) {
    #   # need abs(sum(betas) - s) to be 1 or 0
    #   betas[(s + 1):p] <- as.numeric(sum(betas) < s)
    # }

    return(betas)
  }
  
  if (type == 2) {
    return(c(rep(default, s), rep(0, p - s)))
  }
  
  if (type == 3) {
    return(c(seq(-10, 0.5, length.out = s), 
             rep(0, p - s)))
  }
  
  if (type == 4) {
    return(c(seq(-10, 10, length.out = s), 
             rep(0, p - s)))

  }
  
  if (p == s) {
    return(c(rep(default, s)))
  }
  
  return(c(rep(default, s), 0.5^seq(s + 1, p)))
}
```

```{r rmse}
rmse <- function(predicted, true) {
  return(sqrt(sum((predicted - true)^2) / length(predicted)))
}
```

```{r generate_data, warnings=FALSE}

## DESCRIPTION OF INPUTS
# type, p, s, default are all the arguments that are taken in by generate_beta
# cov_x = covariance between covariates
# mean_x = mean of covariates
# sigma_x = standard deviation of covariate distribution
# mean_r = mean of random intercepts
# sigma_r = standard deviation of random intercepts

generate_data <- function(n_bar, J, alpha, 
                          type, p, s, default,
                          cov_x, scale, base_mean_x = 0, sigma_x = 1, 
                          mean_r = 0, sigma_r, 
                          mean_noise = 0, sigma_noise = 1) {
  
  # generate sizes of each cluster -- can probably make this its own function
  n_min <- round(n_bar * (1 - alpha))
  n_max <- round(n_bar * (1 + alpha))
  sample_size <- sample(n_min:n_max, J, replace = TRUE)
  if (n_min == n_max) {
    sample_size <- rep(n_bar, J)
  }
  n <- sum(sample_size)
  
  colnames_df <- c(paste("X", 1:p, sep = ""), "random_beta") 
  r_b <- rnorm(n = J, mean = mean_r, sigma_r)
  random_beta <- rep(r_b, times = sample_size)
  
  fixed_beta <- generate_beta(type, p, s, default)
  
  sigma <- diag(sigma_x^2 - cov_x, p, p) + matrix(cov_x, nrow = p, ncol = p)
  print(sigma)
  mean_x <- matrix(rep(scale * random_beta, p), nrow = n, ncol = p)
  x_all <- rmvnorm(n = n, mean = rep(base_mean_x, p), sigma)
  
  # decided to scrap this, and add a whole matrix of means instead (repeated across columns)
  # Cholesky decomposition to generate correlated covariates
  # c <- chol(sigma)
  # x_unc <- replicate(p, rnorm(n, mean_x))
  # x_all <- x_unc %*% c
  
  ## TODO 1/6: make separate method to build shifted X (and try to modulate all of the other functions). also check that data-generating process works as desired
  
  x_all <- mean_x + x_all
  measurement_noise <- rmvnorm(n = n, mean = rep(mean_noise, p), 
                               diag(sigma_noise^2, p, p))
  noisy_x <- x_all + measurement_noise
  
  data <- as.data.frame(cbind(noisy_x, random_beta))
  colnames(data) <- colnames_df
  
  # error term 
  epsilon <- rnorm(n)
  # add a comment 1/21 for git testing purposes....
  
  # now, relaxing the assumption of how strong our measures are. per Luke's suggestion,
  # add the noise into the covariates so that we preserve the structure from earlier.
  
  # fb = as.matrix(fixed_beta, nrow = 1) -- to avoid using sweep
  data$Y <- rowSums(sweep(x_all, 2, fixed_beta, "*")) + random_beta + epsilon
  
  data$j <- as.factor(rep(1:J, times = sample_size))
  # print("generated data")
  
  return(data)
}

# check extreme cases (e.g., scale = 20) 
# check things visually for each parameter (think about what happens in extreme cases, that should cause noticeable changes in distribution)

# test_df <- generate_data(n_bar = 20, J = 3, alpha = 0.2,
              # type = 1, p = 5, s = 3, default = 1, 
              # cov_x = 0.5, scale = 1, base_mean_x = 0, sigma_x = 1, 
              # mean_r = 0, sigma_r = 1,
              # mean_noise = 0, sigma_noise = 1)

```

```{r}
library(caret)
perf_metrics <- function(beta_hat, beta_true, y_hat, y_true, fixed) {
   # also want to measure the accuracy of both -- root mean squared error of 
  # beta hat and beta_true
  # print("PREDICTED COEFFICIENTS")
  # print(beta_hat)
  # print('ACUTAL COEFFICIENTS')
  # print(beta_true)
  
  y_rmse <- rmse(y_hat, y_true)
  
  beta_rmse <- rmse(beta_hat, beta_true)
  
  # rewrite beta values as 0 (equal to zero) and 1 (nonzero)
  
  true_beta_categorical <- factor(beta_true != 0, levels = levels(factor(c(TRUE, FALSE))))
  pred_beta_categorical <- factor(beta_hat != 0, levels = levels(factor(c(TRUE, FALSE))))
  
  # print(true_beta_categorical)
  # print(pred_beta_categorical)
  
  cm <- confusionMatrix(pred_beta_categorical, true_beta_categorical,
                mode = "everything",
                positive="TRUE")
  # print(cm)
  
  # previously, true positive/negative
  sensitivity <- unname(cm$byClass["Sensitivity"])
  specificity <- unname(cm$byClass["Specificity"])
  precision <- unname(cm$byClass["Precision"])
  recall <- unname(cm$byClass["Recall"])
  f <- unname(cm$byClass["F1"])
  
  results_list <- list(y_rmse, beta_rmse, sensitivity, specificity, precision, recall, f)
  names <- c("y_rmse", "beta_rmse", "sensitivity", "specificity", "precision", "recall", "F1")
  if (fixed) {
    names(results_list) <- unlist(lapply(names, function(x) paste(x, "_fixed", sep = "")))
  }
  
  else {
    names(results_list) <- unlist(lapply(names, function(x) paste(x, "_mixed", sep = "")))
  }
  
  return(results_list)
}
```


```{r}
# Analyze
analyze <- function(data, p, true_beta) {
  # need to do cross validation for LASSO
  var.names <- paste("X", 1:p, sep = "")
  com.names <- lapply(seq_along(var.names),
                    function(i) combn(var.names, i, FUN = paste, collapse = " + "))
  covariates <- com.names[[p]]
  
  # indi <- sample(0.80 * nrow(data), 1:nrow(data), replace = FALSE)
  # 
  # data_train <- data[-indi,]
  # data_test <-data[indi,]
  
  X_fixed <- model.matrix(as.formula(paste("Y ~", covariates)),
                          data = data)
  X_fixed <- X_fixed[, -1]

  # call cv.glmnet()
  model_lasso_fixed <- cv.glmnet(x = X_fixed, y = data$Y, nfolds = 5, alpha = 1)
  coefficients_fixed <- coef(model_lasso_fixed, s = model_lasso_fixed$lambda.min)[2:(length(true_beta) + 1)]
  # return(glmnet(X_fixed, data$Y, alpha = 1,
  #               lambda = model_lasso_fixed$lambda.min))
  
  lambda_min <- cv_glmmLasso(data = data,
                             formula = as.formula(paste("Y ~", covariates)))
  # print(lambda_min) 
  model_lasso_mixed <- glmmLasso(fix = as.formula(paste("Y ~", covariates)),
                                 rnd=list(j=~1),
                                 data = data,
                                 lambda=lambda_min,
                                 final.re = TRUE)
  coefficients_mixed <- model_lasso_mixed$coefficients[2:(length(true_beta) + 1)]
  # print(coefficients_fixed)
  # print(coefficients_mixed)
  # coefficients <- cbind(as.matrix(coefficients_fixed), as.matrix(coefficients_mixed))
  # colnames(coefficients) <- c("fixed", "mixed")
  # coefs_df <- as.data.frame(coefficients[2:nrow(coefficients), ])
  
  # report R^2 as a measure of signal to noise ratio
  
  basic_linear <- lm(as.formula(paste("Y ~", covariates)), data = data)
  
  # "the two models agreed on ___ percent of the covariates" -- agreement defined as they both 
  # deemed a beta as non-zero out of the total significant betas. not really sure if this
  # is legitimate though
  
  # do I need to use train and test for the agreement? no, right? (because I'm only fitting one
  # model)?
  
  agreement <- sum((coefficients_fixed != 0 & coefficients_mixed != 0) |
                  (coefficients_fixed == 0 & coefficients_mixed == 0)) / p
  
  y_hat_fixed <- predict(model_lasso_fixed, newx = X_fixed, s = model_lasso_fixed$lambda.min)
  y_hat_mixed <- predict(model_lasso_mixed, newx = X_fixed, s = lambda_min)
  
  fixed_results <- perf_metrics(coefficients_fixed, true_beta, y_hat_fixed, data$Y, TRUE)
  mixed_results <- perf_metrics(coefficients_mixed, true_beta, y_hat_mixed, data$Y, FALSE)
  
  # TODO 1/9: consider adding intra and inter-cluster variation, as well as variation (variance?) 
  # explained, alongside the adjusted r squared
  
  return(c(list("adj_r_squared" = summary(basic_linear)$adj.r.squared, "agreement" = agreement), 
           fixed_results,
           mixed_results))
}

# test_beta <- generate_beta(1, 5, 3)
# results <- analyze(test_df, 5, test_beta)

```


```{r}
# Repeat

one_run <- function(n_bar, J, alpha, 
                    type, p, s, default,
                    cov_x, scale, base_mean_x = 0, sigma_x = 1, 
                    mean_r = 0, sigma_r, 
                    mean_noise = 0, sigma_noise = 1) {
  
  dat <- generate_data(n_bar, J, alpha, 
                    type, p, s, default,
                    cov_x, scale, base_mean_x = 0, sigma_x = 1, 
                    mean_r = 0, sigma_r, 
                    mean_noise = 0, sigma_noise = 1)
  # print(dat)
  
  fixed_beta <- generate_beta(type, p, s, default)
  # may need to fix this
  result <- analyze(dat, p, fixed_beta)
  # feed back number of predictors, number of clusters, type of fixed beta coefficients (from paper)
  # the default value of beta, sigma of the random intercepts, number of 
  # return(c(sample_size, 
  #           type, p, s, default,
  #           cov_x, scale, base_mean_x, sigma_x, 
  #           mean_r, sigma_r, 
  #           mean_noise, sigma_noise, result))
  
  return(result)
}

# sigma_r -- clusters are further and further apart
# first step -- "the clusters are just adding noise" -- have weak beta (slopes) and alter sigma_r
# eventually work up to using covariates directly from data
# next step -- correlate the intercepts with the covariates -- do this after you get the whole simulation going

# if you keep getting weird results with the simulation, email dataset and short script to groll@statistik.tu-dortmund.de
```


```{r}
rerun_single_params <- function(df) {
  print("starting an iteration")
  # print(df[7])
  
  results <- rerun(unname(df[1]), one_run(n_bar = unname(df[2]), J = unname(df[3]), alpha = unname(df[4]), 
                                          type = unname(df[5]), p = unname(df[6]), 
                                          s = unname(df[7]), default = unname(df[8]),
                                          cov_x = unname(df[9]), scale = unname(df[10]), 
                                          base_mean_x = unname(df[11]), sigma_x = unname(df[12]),
                                          mean_r = unname(df[14]), sigma_r = unname(df[15]), 
                                          mean_noise = unname(df[16]), sigma_noise = unname(df[17])))
  
  results_length <- length(results[[1]])
  
  mean_results <- sapply(1:results_length, 
                         function(j) mean(unlist(lapply(results, function(x) x[j]))))
  sd_results <- sapply(1:results_length, 
                         function(j) sd(unlist(lapply(results, function(x) x[j]))))
  
  row <- c(sapply(1:length(df), function(x) unname(df[x])), mean_results, sd_results)
  # print(row)
  # print(unname(sapply(names(results[[1]]), function(x) paste("mean_", x, sep = ""))))
  
  names(row) <- c(names(df), 
                  unname(sapply(names(results[[1]]), function(x) paste("mean_", x, sep = ""))),
                  unname(sapply(names(results[[1]]), function(x) paste("sd_", x, sep = ""))))

  return(row)
}
```


```{r}
rerun_mult_parameters <- function(runs_vec, n_bar_vec, J_vec, alpha_vec,
                                  type_vec, p_vec, s_vec, default_vec,
                                  cov_x_vec, scale_vec, base_mean_x_vec, sigma_x_vec,
                                  mean_r_vec, sigma_r_vec, 
                                  mean_noise_vec, sigma_noise_vec) {
  
  ## NEED TO FIX THIS BECAUSE WE ARE NOW PASSING IN A LIST AS A PARAMETER -- SOMETHING TO ASK LUKE ABOUT? 
  ## TAKE A LOOK AT THE SIMULATION MANUAL AS WELL
  
  ## current suggestion of a solution -- pass in string version of sample size. convert to vector later
  ## ^^ this is kind of roundabout so look into other solutions before finalizing
  
  combos <- expand.grid(runs = runs_vec, n_bar = n_bar_vec, J = J_vec, alpha = alpha_vec,
                        type = type_vec, p = p_vec, s = s_vec, default = default_vec,
                        cov_x = cov_x_vec, scale = scale_vec, 
                        base_mean_x = base_mean_x_vec, sigma_x = sigma_x_vec,
                        mean_r = mean_r_vec, sigma_r = sigma_r_vec, 
                        mean_noise = mean_noise_vec, sigma_noise = sigma_noise_vec)
  
  results <- apply(combos, 1, rerun_single_params)
  print("finished one set of parameters")
  
  return(results)
  
}



# Summarize
assess_performance <- function(results) {
  # stuff
  return(as.data.frame(t(results)))
}
```


```{r}
library(tictoc)
options(warn=-1)
set.seed(17)
plan(multisession, workers = parallel::detectCores() - 1)

params <- list(runs_vec = 10,
               n_bar_vec = 50,
               J_vec = 5,
               alpha_vec = 0.5,
               type_vec = 1,
               p_vec = 8,
               s_vec = 4,
               default_vec = 1,
               cov_x = 0.5,
               scale_vec = 1,
               base_mean_x_vec = 0,
               sigma_x_vec = 1,
               mean_r_vec = 0,
               sigma_r_vec = 2,
               mean_noise_vec = 0,
               sigma_noise_vec = 1)

tic("10 iterations one set")
one_set <- future_pmap(params, rerun_mult_parameters)
Sys.sleep(1)
toc()
# takes 50 seconds for 10 iterations of one set, with 1 core?
# takes 57 seconds for 10 iterations of one set, with 6 cores?? doesn't make sense lol

one_set_results <- assess_performance(one_set)
# write.csv(one_set_results, "/n/home04/thupham17/fasrc/one_set_results.csv")
```



```{r}
# options(warn=-1)
# set.seed(17)
# 
# prelim_results <- rerun_mult_parameters(runs_vec = 100, 
#                                         n_bar_vec = seq(from = 20, to = 100, by = 20),
#                                         J_vec = seq(from = 2, to = 10, by = 2), 
#                                         alpha_vec = seq(from = 0, to = 1, 
#                                                         by = 0.25),
#                                         type_vec = 1:5,
#                                         p_vec = seq(from = 2, to = 20, by = 6), 
#                                         s_vec = seq(from = 1, to = 20, by = 6), 
#                                         default_vec = 1,
#                                         cov_x = seq(from = 0, to = 6, by = 3),
#                                         scale_vec = seq(from = 0, to = 6, by = 3),
#                                         base_mean_x_vec = 0, 
#                                         sigma_x_vec = 1,
#                                         mean_r_vec = 0, 
#                                         sigma_r_vec = seq(from = 0, to = 4, by = 2),
#                                         mean_noise_vec = 0, 
#                                         sigma_noise_vec = 1)
```

```{r}
# results <- assess_performance(prelim_results)
# write.csv(results, "/n/home04/thupham17/fasrc/results.csv")
```

```{r}
# Profiling R Code
# to profile code with system time method, etc.

# quickadd <- function(g){
# *return(g+1)*
# *}*
# 
# *slowadd <- function(g){*
# *h <- rep(NA, length(g))*
# *for (i in 1:length(g)){*
# *h[i] <- g[i] + 1*
# *}*
# *return(h)*
# *}*
# 
# *system.time(a <- slowadd(g))*
# user system elapsed
# 0.34 0.00 0.34
# 
# *system.time(a <- quickadd(g))*
# user system elapsed
# 0 0 0

#code
```

