---
title: "simulation_scratchwork"
author: "Thu Pham"
date: "2022-11-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/Users/thupham/Desktop/senior-yr/thesis/elsah-coptop')
library(readstata13)
library(tidyverse)
library(glmnet)
library(caret)
library(fastDummies)
library(mvtnorm)
library(dplyr)
library(glmnet)
library(glmmLasso)
```


```{r}
# helper functions


# need to find a minimum lambda value that we can start from, since too small of a lambda value returns a message about the matrix not being invertible
# min_lambda <- function(data, formula, rand=list(j=~1)) {
#   failed <- TRUE
#   lambda_min <- 0.01
#   repeat {
# 
#   failed <- tryCatch(
#       {
#         glmmLasso(fix = formula, rnd = rand, data = data, 
#             lambda = lambda_min)
#       },
#       error = function(e){
#           TRUE
#       }
#    )
#     if (is.logical(failed)) {
#       lambda_min <- lambda_min + 0.01
#     }
#     
#     else {
#       break
#     }
#   }
#   return(lambda_min)
# }
```


```{r}
# cross validation helper function for the glmmLasso
# need to eventually do this with k-folds, or get as close to the 
# cross validation that is happening in the regular lasso

library(MASS)
library(nlme)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# https://rdrr.io/cran/glmmLasso/src/demo/glmmLasso-soccer.r
cv_glmmLasso <- function(data, formula, rand=list(j=~1),
                         lambda_step=100) {
  # fixing the min lambda for now -- let's see if that solve our problem
  # min_lambda <- 0.001 
  
  # lambdas <- seq(from=min_lambda, to=10, 
                 # by=(10 - min_lambda) / lambda_step)
  lambdas <- 10 ^ seq(10,-2,length = 100)
  # print(lambdas)
  
  N <- dim(data)[1]
  ind <- sample(N, N)
  
  # number of folds
  
  kk <- 5
  nk <- floor(N/kk)
  
  pred_error <- matrix(NA, ncol=kk, nrow = lambda_step)

  
  for (j in 1:lambda_step) {
    # print(paste("Iteration", j))
    
    for (i in 1:kk)
    {
      if (i < kk) {
      indi <- ind[(i-1)*nk+(1:nk)]
      }
      else {
      indi <- ind[((i-1)*nk+1):N]
      }
  
    data_train <- data[-indi,]
    data_test <-data[indi,]
  
    glm <- try(glmmLasso(fix = formula, rnd = rand, data = data_train, 
              lambda = lambdas[j]), silent = TRUE) 
            
        if(!inherits(glm, "try-error"))
        {  
          y_hat<-predict(glm, data_test)  
          pred_error[j,i]<- sqrt(sum((data_test$Y - y_hat)^2) / nrow(data_test))
        }
    }
  }
  
  # find lambda which gives lowest prediction error
  pred_error_vec <- apply(pred_error, 1, sum)
  # print(pred_error_vec)
  # print(pred_error_vec)
  # print(paste("min lambda:", lambdas[which.min(pred_error_vec)]))
  return(lambdas[which.min(pred_error_vec)])  
}

cv_glmmLasso(data, Y ~ X1 + X2 + X3 + X4 + X5)
```

```{r, warnings=FALSE}
# Generate
# m -- number of covariates, j -- number of clusters, sigma_r -- variance of 
# different intercepts

# beta strength -- 0 to 1, indicates the proportion of beta coefficient that is 1

## TO DO: NEED TO INTRODUCE SOME WAY TO CORRELATE THE CLUSTERS WITH THE BETA COEFFICIENTS

## TO DO: NEED TO FIND SOME WAY TO MAKE UNEVENLY SIZED CLUSTERS

## TO DO: different strengths of beta coefficients?

generate_data <- function(m, j, beta_strength, min_beta, sigma_r, n=100) {
  colnames_df <- c(paste("X", 1:m, sep = ""), "random_beta") 
  random_beta <- rnorm(n=j, mean=0, sigma_r)
  
  fixed_beta <- c(rep(min_beta, (1 - beta_strength) * m), rep(1, beta_strength*m))
  # print(fixed_beta)
  df <- matrix(nrow=n)
  # generate X_j
  for (i in 1:m) {
    df <- cbind(df, rnorm(n))
  }
  # first column is NA for some reason, definitely need to clean this up later.
  df <- df[, -1]
  data <- as.data.frame(df)
  # make this more efficient later
  data <- cbind(data, rep(random_beta, each=n/j))
  colnames(data) <- colnames_df
  # error term
  epsilon <- rnorm(n)
  data$Y <- rowSums(sweep(data[, 1:m], 2, fixed_beta, "*")) + random_beta + epsilon
  data$j <- as.factor(rep(1:j, each=n/j))
  # print("generated data")
  return(data)
}

```



```{r}
# Analyze
analyze <- function(data, m, fixed_beta) {
  # need to do cross validation for LASSO
  var.names <- paste("X", 1:m, sep = "")
  com.names <- lapply(seq_along(var.names),
                    function(i) combn(var.names, i, FUN = paste, collapse = " + "))
  covariates <- com.names[[m]]
  X_fixed <- model.matrix(as.formula(paste("Y ~", covariates)),
                          data = data)
  X_fixed <- X_fixed[, -1]
  Y <- data$Y

  # call cv.glmnet()
  model_lasso_fixed <- cv.glmnet(x = X_fixed, y = Y, nfolds = 5, alpha = 1)
  cc <- coef(model_lasso_fixed, s = model_lasso_fixed$lambda.min)
  #placeholder lambda value
  coefficients_fixed <- coef(model_lasso_fixed)
  # print(paste("fixed coefficients:", coefficients_fixed))
  
  
  lambda_min <- cv_glmmLasso(data = data,
                             formula = as.formula(paste("Y ~", covariates)))
  model_lasso_mixed <- glmmLasso(fix = as.formula(paste("Y ~", covariates)),
                                                     rnd=list(j=~1),
                                                    data = data,
                                                     lambda=lambda_min)

  coefficients_mixed <- model_lasso_mixed$coefficients
  # print("generated coefficients")
  coefficients <- cbind(as.matrix(coefficients_fixed), as.matrix(coefficients_mixed))
  colnames(coefficients) <- c("fixed", "mixed")
  coefs_df <- as.data.frame(coefficients[2:nrow(coefficients), ])
  
  # "the two models agreed on ___ percent of the covariates 
  
  agreement <- sum(coefs_df$fixed !=0 & coefs_df$mixed !=0) / nrow(coefs_df)
  
  # also want to measure the accuracy of both
  
  fixed_accuracy <- sqrt(sum((coefs_df$fixed - fixed_beta)^2) / nrow(coefs_df))
  mixed_accuracy <- sqrt(sum((coefs_df$mixed - fixed_beta)^2) / nrow(coefs_df))
  
  return(c(agreement, fixed_accuracy, mixed_accuracy))
}

```


```{r}
# Repeat
one_run <- function(m, j, beta_strength, min_beta, sigma_r, n=100) {
  dat <- generate_data(m, j, beta_strength, min_beta, sigma_r, n)
  fixed_beta <- c(rep(min_beta, beta_strength*m), rep(1, (1 - beta_strength) * m))
  result <- analyze(dat, m, fixed_beta)
  return(c(m, j, beta_strength, min_beta, sigma_r, n, result))
}

# sigma_r -- clusters are further and further apart
# first step -- "the clusters are just adding noise" -- have weak beta (slopes) and alter sigma_r
# eventually work up to using covariates directly from data
# next step -- correlate the intercepts with the covariates -- do this after you get the whole simulation going

# if you keep getting weird results with the simulation, email dataset and short script to groll@statistik.tu-dortmund.de

rerun_single_params <- function(df) {
  # print("starting an iteration")
  # print(df[7])
  results <- rerun(unname(df[1]), one_run(m = unname(df[2]), 
                                          j = unname(df[3]), 
                                          beta_strength = unname(df[4]),
                                          min_beta = unname(df[5]), 
                                          sigma_r = unname(df[6]),
                                          n = unname(df[7])))
  mean_agreement <- mean(unlist(lapply(results, function (x) x[7])))
  sd_agreement <- sd(unlist(lapply(results, function (x) x[7])))
  
  mean_accuracy_fixed <- mean(unlist(lapply(results, function (x) x[8])))
  sd_accuracy_fixed <- sd(unlist(lapply(results, function (x) x[8])))
  
  mean_accuracy_mixed <- mean(unlist(lapply(results, function (x) x[9])))
  sd_accuracy_mixed <- sd(unlist(lapply(results, function (x) x[9])))
  
  row <- c(unname(df[2]), unname(df[3]), unname(df[4]), unname(df[5]), 
           unname(df[6]), unname(df[7]), unname(df[1]), 
           mean_agreement, sd_agreement, mean_accuracy_fixed, sd_accuracy_fixed,
           mean_accuracy_mixed, sd_accuracy_mixed)
  names(row) <- c("m", "j", "beta_strength", "min_beta", "sigma_r", "n", "reps",
                  "mean_agreement", "sd_agreement", "mean_acc_fixed", "sd_acc_fixed",
                  "mean_acc_mixed", "sd_acc_mixed")
  
  return(row)
}


rerun_mult_parameters <- function(runs_vec, m_vec, j_vec, beta_strength_vec, 
                                  min_beta_vec, sigma_r_vec, n_vec) {
  combos <- expand.grid(runs = runs_vec, m = m_vec, j = j_vec, 
                        beta_strength = beta_strength_vec, 
                        min_beta = min_beta_vec, sigma_r = sigma_r_vec, n = n_vec)
  results <- apply(combos, 1, rerun_single_params)
  print("finished one set of parameters")
  
  return(results)
  
}

# test_results <- rerun_mult_parameters(runs_vec = 2, m_vec = c(2, 4), j_vec = 4,
#                       beta_strength_vec = 0.5, min_beta_vec = 0.4, 
#                       sigma_r_vec = 0.2, n_vec = 100)



# Summarize
assess_performance <- function(results) {
  # stuff
  return(as.data.frame(t(results)))
}
```

```{r}

# this took a really long time to run, so maybe wait for a bit
# prelim_results <- rerun_mult_parameters(runs_vec = 100, 
#                                         m_vec = seq(from=4, to=40, by=4),
#                                         j_vec = c(2, 4, 5, 10, 25, 50),
#                                         beta_strength_vec = c(0.25, 0.50, 0.75),
#                                         min_beta_vec = seq(from=0.1, to=0.8, by=0.1),
#                                         sigma_r_vec = seq(from=0.1, to=6, by=0.5),
#                                         n_vec = c(50, 100, 200))

set.seed(17)

# prelim_results <- rerun_mult_parameters(runs_vec = 5, 
#                                         m_vec = seq(from=4, to=20, by=8),
#                                         j_vec = c(2, 10),
#                                         beta_strength_vec = c(0.25, 0.50),
#                                         min_beta_vec = seq(from = 0.1, to = 0.9, 
#                                                            by = 0.4),
#                                         sigma_r_vec = seq(from = 1, to = 5, by = 2),
#                                         n_vec = 100)

prelim_results <- rerun_mult_parameters(runs_vec = 5,
                                        m_vec = c(4, 8),
                                        j_vec = 2,
                                        beta_strength_vec = 0.25,
                                        min_beta_vec = 0.1,
                                        sigma_r_vec = 1,
                                        n_vec = 100)

assess_performance(prelim_results)
```

